# 深度学习笔记
## 1、引言
### 深度学习
为什么深度学习突然间这么好使了？

$$
signmoid:\quad\quad \sigma(x) = \frac{1}{1 + e^{-x}}
$$

$$
ReLU:\quad\quad \sigma(x) = \max(0, x)
$$

$$
tanh(x):\quad\quad\  tanh(x) = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}
$$



![激活函数](https://picx.zhimg.com/70/v2-2a3ed1781c38d42fed831039512a22d4_1440w.image?source=172ae18b&biz_tag=Post)

## 2、回归问题

### 二元分类

问题引入：

一些符号约定：
$$
(x,y):\quad单个样本\qquad x\in R^{n_x}\qquad y\in \{0,1\}\\
m \text{训练集的样本总数  train example}:\quad(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)})\dots(x^{(m)},y^{(m)}),\\有时特殊表示,m_{train}或者m_{test}\\
\begin{align}
X &= \{ x^{(1)},  x^{(2)}\dots  x^{(m)}\}\quad X.shape()：n_x\times m\\
Y &= \{ y^{(1)},  y^{(2)}\dots  y^{(m)}\}\quad Y.shape()：1\times m\\
\end{align}
$$

### 逻辑回归

给定x ，计算在该条件下（x）,y=1的概率


$$
Given\quad x\quad x\in R^{n_x},want\quad y^{\wedge} = P(y=1|x)\\
Parameters:\quad w\in R^{n_x},\quad b\in R\\
Output:y^{\wedge} = w^Tx+b
$$
上述算法很难实现这个要求，因为上述Output的值可能会比1大很多或者是一个负数，对于概率会失去意义。在使用sigmoid函数后，因此：
$$
Output:y^{\wedge} = \sigma(w^Tx+b)\\
附：sigmoid:\quad\sigma(z) = \frac{1}{1 + e^{-z}}
$$
当z非常大时，&sigma;（z）的值接近于1，反之，接近于0

### 逻辑损失代价函数


$$
y^{{\wedge}^{(i)}} = \sigma(w^Tx^{(i)}+b)\quad where\quad \sigma(z^{(i)}) = \frac{1}{1 + e^{-z^{(i)}}} \quad z^{(i)}=w^Tx^{(i)}+b\\
Given\quad\{(x^{(1)},y^{(1)})\dots(x^{(m)},y^{(m)})\}\quad want\quad y^{{\wedge}^{(i)}}≈y^{(i)}
$$

#### 损失函数

Loss(error) function:
$$
L(y^{\wedge},y) = -(y*logy^{\wedge}+(1-y)*log(1-y^{\wedge})
$$
理解：

if y=1 : L =  -log y<sup>^</sup>			<--想要 log y<sup>^</sup>足够小（即损失函数L）接近于0，则 y<sup>^</sup>足够接近于1(已经过sigmoid函数)。

if y = 0: L= -log(1- y<sup>^</sup>)		<--想要损失函数值变小，则y<sup>^</sup>要尽可能变小

拉裴拉效应：在要保证损失函数的值尽可能小的结果下（->0），如果y=1,y<sup>^</sup>的值要变大<big>&uarr;</big>；	如果y=0，y<sup>^</sup>的值要变小<big>&darr;</big>

#### 代价函数

所有损失函数求和后的均值

Cost function:
$$
\begin{align}
J(w,b) &= \frac{1}{m}\sum_{i=1}^{m}L(y^{{\wedge}^{(i)}},y^{(i)})\\
&= - \frac{1}{m}[(y^{(i)}*logy^{{\wedge}^{(i)}}+(1-y^{(i)})*log(1-y^{{\wedge}^{(i)}}]
\end{align}
$$
在优化逻辑回归模型的时，我们要试着去找参数W和B，以此来缩小J的整体成本

### 梯度下降

通过梯度下降的方式，使得`损失函数 J(W,B) 的值越来越小`（在此处为了方便作图表示，仅W与B两个变量）



 以其中一个变量（此处为W）为例，说明其`迭代方法`
$$
w:=w-\alpha\frac{dJ(w)}{dw}
$$


为了更常见的，符合微积分符号规定的写法，改为：
$$
w:=w-\alpha\frac{\alpha J(w,b)}{\alpha w}
$$
同理
$$
b:=b-\alpha\frac{\alpha J(w,b)}{\alpha b}
$$
一般地，我们在写代码时，记
$$
w:=w-dw\\
b:=b-db
$$


### 计算图

Computation Graph:

前向传播：

从左往右计算

反向传播：

从右往左运算

（接地气理解：在本单元中、即复合函数求导）

### 逻辑回归梯度下降


$$
z = w^Tx+b\\
y^{\wedge} = a = \sigma(z)\\
L(a,y) =-(y*log(a)+(1-y)*log(1-(a))
$$
利用方向传播进行复合函数求导后（链式法则）：

(为了方便区分，简写的记号用形如da'与求导符号中的da进行区别)
$$
\begin{align}
da' &= \frac{dL(a,y)}{da}=-\frac{y}{a}+\frac{1-y}{1-a}\\
dz' &= \frac{dL}{dz}=\frac{dL}{da}*\frac{da}{dz}=da'*a(1-a) = a-y\\
dw_1' &= x_1*dz'\\
dw_2' &= x_2*dz'\\
db' &=dz;
\end{align}
$$
### m示例上的梯度下降

一次迭代：

```abstract
J=0  dw_1=0  dw_2=0  db=0
for i = 1 to m:
	z^(i) = w.t * x^(i) + b
	a^(i) = σ(z^(i))	#σ()本例中为sigmoid函数
	J += -[y^(i)loga^(i)+(1-y^(i))log(1-a^(i))]
	dz^(i) = a^(i) - y^(i)
	dw_1 += x^(i)_1 * dz^(i)
	dw_2 += x^(i)_2 * dz^(i)
	db += dz^(i)
J /= m
dw_1 /= m  dw_2 /= m  db /= m
w_1 := w1 - αdw_1	#α为学习率
w_2 := w2 - αdw_2
b := b - αdb
```



存在问题：

- 两重for循环
  - 用于m训练样本
  - 计算所有特征（dw_i）

使用向量化优化，提高计算速度（并行计算）

### 向量化

Vectorization:

~~~python
```
Z = w.t * x + b
w.shape = x.shape = n_x × 1
```

#Non-Vectorized:
z = 0
for i in range(n_x):
	z+=w[i]*x[i]
z+=b


#Vectoried:
z = np.dot(w,x)+b		# w.t * x + b

```
在jupyter booknote中shift+enter运行
import numpy as np
a = np.array([1,2,3,4])
print(a)
-------------------
57
```
~~~



CPU和GPU都有并行化的指令，有时叫做SIMD指令，这代表一个单独指令多维数据。

使得利用并行化去更快的计算。

### 更多向量化示例

`Whenever possible,avoid explicit for-loops.`



```python
#v向量的每个元素x变成其e^x,在u表示
#Non-V
u = np.zeros((n,1))
for i in range(n):
    u[i] = math.exp(v[i])
#V
import numpy as np
u = np.exp(v)
---------------------------------
np.log()
np.abs()
np.maxnum(v,0)
v**2
1/v

```

初始代码：



改进计算所有特征（消灭第二个for循环）：



```abstract
J = 0 dw = np.zeros(n_x,1) db = 0
for i = 1 to m:
	z^(i) = w.t * x^(i) + b
	a^(i) = σ(z^(i))	#σ()本例中为sigmoid函数
	J += -[y^(i)loga^(i)+(1-y^(i))log(1-a^(i))]
	dz^(i) = a^(i) - y^(i)
	dw += x^(i)*dz^(i)	//在代码中采取np.dot(x^(i),dz^(i))
	db += dz^(i)
J /= m	dw /=m  db /= m
dw := dw - αdw	#α为学习率
b := b - αdb
```

### 向量化逻辑回归

原：



改进：
$$
X = \{ x^{(1)},  x^{(2)}\dots  x^{(m)}\}\quad X.shape()：n_x\times m\\\\
\begin{align}
Z=[z^{(1)},  z^{(2)}\dots  z^{(m)}]&=w^TX+[b,b\dots b]\\
&=[w^Tx^{(1)}+b,w^Tx^{(2)}+b\dots w^Tx^{(m)}+b]\\
&=[z^{(1)},z^{(2)}\dots z^{(m)}]
\end{align}
$$

```python
Z = np.dot(w.T,X)+b
```

$$
A = [a^{(1)},a^{(2)}\dots a^{(m)}]
$$

### 向量化逻辑回归的梯度输出

$$
dz^{(1)} = a^{(1)} - y^{(1)}\quad dz^{(2)} = a^{(2)} - y^{(2)}\dots\\
dZ=[dz^{(1)},dz^{(2)}\dots dz^{(m)}]\\
A = [a^{(1)},a^{(2)}\dots a^{(m)}]\qquad Y = [y^{(1)},y^{(2)}\dots y^{(m)}]\\
dZ  = A - Y = [a^{(1)}-y^{(1)},a^{(2)}-y^{(2)}\dots a^{(m)}-y^{(m)}]
$$


$$
\begin{align}
db &= \frac{1}{m}\sum_{i=1}^{m}dz^{(i)}\\
&=\frac{1}{m}np.sum(dz)
\end{align}
$$

$$
dw = \frac{1}{m}Xdz^T
=\frac{1}{m}[x^{(1)}\dots x^{(m)}]
\begin{bmatrix}
    dz^{(1)} \\
   	dz^{(2)} \\
   	:		\\
    dz^{(m)} \\
\end{bmatrix}
$$
