


# 深度学习笔记
## 1、引言
### 深度学习
$$
\text{signmoid:}\quad\quad \sigma(x) = \frac{1}{1 + e^{-x}}
$$

$$
\text{ReLU:}\quad\quad \sigma(x) = \max(0, x)
$$

$$
\text{tanh(x):}\quad\quad\ \tanh(x) = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}
$$

![激活函数](https://picx.zhimg.com/70/v2-2a3ed1781c38d42fed831039512a22d4_1440w.image?source=172ae18b&biz_tag=Post)

## 2、回归问题

### 二元分类

问题引入：

一些符号约定：
$$
\begin{aligned}
(x,y):\quad &\text{单个样本} \quad x \in \mathbb{R}^{n_x} \quad y \in \{0,1\} \\
m \quad &\text{训练集的样本总数} \quad \text{train example}: (x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}), \dots, (x^{(m)},y^{(m)}) \\
&\text{有时特殊表示: } m_{\text{train}} \text{ 或者 } m_{\text{test}} \\
X &= \{ x^{(1)},  x^{(2)}, \dots,  x^{(m)}\} \quad X.shape()：n_x \times m \\
Y &= \{ y^{(1)},  y^{(2)}, \dots,  y^{(m)}\} \quad Y.shape()：1 \times m
\end{aligned}
$$


### 逻辑回归

给定 $$x$$ ，计算在该条件下 $$(x)$$, $$y=1$$ 的概率：

$$
\text{Given}\quad x \quad x \in \mathbb{R}^{n_x}, \text{ want } \hat{y} = P(y=1|x) \\
\text{Parameters:}\quad w \in \mathbb{R}^{n_x}, \quad b \in \mathbb{R} \\
\text{Output:} \quad \hat{y} = w^T x + b
$$

上述算法很难实现这个要求，因为上述输出值可能会比1大很多或者是一个负数，对于概率会失去意义。在使用sigmoid函数后，因此：

$$
\text{Output:} \quad \hat{y} = \sigma(w^T x + b) \\
\text{附: } \text{sigmoid:} \quad \sigma(z) = \frac{1}{1 + e^{-z}}
$$

当 $$z$$ 非常大时，$$\sigma(z)$$ 的值接近于1，反之，接近于0。

### 逻辑损失代价函数

$$
\hat{y}^{(i)} = \sigma(w^T x^{(i)} + b) \quad \text{where} \quad \sigma(z^{(i)}) = \frac{1}{1 + e^{-z^{(i)}}} \quad z^{(i)}=w^T x^{(i)}+b \\
\text{Given} \quad \{(x^{(1)},y^{(1)}), \dots, (x^{(m)},y^{(m)})\} \quad \text{want} \quad \hat{y}^{(i)} ≈ y^{(i)}
$$

#### 损失函数

$$
L(\hat{y},y) = -(y \log \hat{y} + (1-y) \log (1-\hat{y}))
$$

理解：

- 如果 $$y=1$$: $$L =  -\log \hat{y}$$，想要 $$\log \hat{y}$$ 足够小（即损失函数 $$L$$ 接近于0），则 $$\hat{y}$$ 足够接近于1。
- 如果 $$y=0$$:$$L = -\log(1- \hat{y})$$，想要损失函数值变小，则 $$\hat{y}$$ 要尽可能变小。

#### 代价函数

所有损失函数求和后的均值：

$$
\begin{align}
J(w,b) &= \frac{1}{m} \sum_{i=1}^{m} L(\hat{y}^{(i)}, y^{(i)}) \\
&= - \frac{1}{m} \left[ y^{(i)} \log \hat{y}^{(i)} + (1-y^{(i)}) \log (1-\hat{y}^{(i)}) \right]
\end{align}
$$

在优化逻辑回归模型时，我们要试着去找参数 $$w$$ 和 $$b$$，以此来缩小 $$J$$ 的整体成本。

### 梯度下降

通过梯度下降的方式，使得损失函数 $$J(w,b)$$ 的值越来越小（在此处为了方便作图表示，仅$$w$$ 与 $$b$$ 两个变量）：

以其中一个变量（此处为 $$w$$）为例，说明其迭代方法：

$$
w := w - \alpha \frac{dJ(w)}{dw}
$$

为了更常见的、符合微积分符号规定的写法，改为：

$$
w := w - \alpha \frac{\partial J(w,b)}{\partial w}
$$

同理：

$$
b := b - \alpha \frac{\partial J(w,b)}{\partial b}
$$

一般地，我们在写代码时，记为：

$$
w := w - dw \\
b := b - db
$$

### 计算图

Computation Graph:

- **前向传播**: 从左往右计算。
- **反向传播**: 从右往左运算（在本单元中，即复合函数求导）。

### 逻辑回归梯度下降

$$
z = w^T x + b \\
\hat{y} = a = \sigma(z) \\
L(a,y) = -(y \log(a) + (1-y) \log(1-a))
$$

利用反向传播进行复合函数求导后（链式法则）：

为了方便区分，简写的记号用形如 `da'` 与求导符号中的 `da` 进行区别：

$$
\begin{align}
da' &= \frac{dL(a,y)}{da} = -\frac{y}{a} + \frac{1-y}{1-a} \\
dz' &= \frac{dL}{dz} = \frac{dL}{da} \cdot \frac{da}{dz} = da' \cdot a(1-a) = a-y \\
dw_1' &= x_1 \cdot dz' \\
dw_2' &= x_2 \cdot dz' \\
db' &= dz'
\end{align}
$$

### m示例上的梯度下降

一次迭代：

```python
J = 0
dw_1 = 0
dw_2 = 0
db = 0
for i = 1 to m:
    z^(i) = w.T * x^(i) + b
    a^(i) = σ(z^(i))  # σ() 本例中为 sigmoid 函数
    J += -[y^(i) log a^(i) + (1-y^(i)) log(1-a^(i))]
    dz^(i) = a^(i) - y^(i)
    dw_1 += x^(i)_1 * dz^(i)
    dw_2 += x^(i)_2 * dz^(i)
    db += dz^(i)
J /= m
dw_1 /= m
dw_2 /= m
db /= m
w_1 := w1 - α dw_1  # α 为学习率
w_2 := w2 - α dw_2
b := b - α db
```

存在问题：

- 两重 `for` 循环
  - 用于 $$m$$ 个训练样本
  - 计算所有特征（$$dw_i$$）

使用向量化优化，提高计算速度（并行计算）。

### 向量化

Vectorization:

```python
# 非向量化
z = 0
for i in range(n_x):
    z += w[i] * x[i]
z += b

# 向量化
z = np.dot(w, x) + b  # w.T * x + b
```

在Jupyter Notebook中 `shift+enter` 运行：

```python
import numpy as np
a = np.array([1,2,3,4])
print(a)
```

CPU和GPU都有并行化的指令，有时叫做SIMD指令，这代表一个单独指令多维数据，使得利用并行化去更快

地完成任务。



### 更多向量化示例

`Whenever possible, avoid explicit for-loops.`

```python
#v向量的每个元素x变成其e^x,在u表示
#Non-V
u = np.zeros((n,1))
for i in range(n):
    u[i] = math.exp(v[i])
#V
import numpy as np
u = np.exp(v)
---------------------------------
np.log()
np.abs()
np.maxnum(v,0)
v**2
1/v
```

初始代码：

改进计算所有特征（消灭第二个for循环）：

```abstract
J = 0 dw = np.zeros(n_x,1) db = 0
for i = 1 to m:
    z^(i) = w.t * x^(i) + b
    a^(i) = σ(z^(i))  # σ()本例中为sigmoid函数
    J += -[y^(i)loga^(i)+(1-y^(i))log(1-a^(i))]
    dz^(i) = a^(i) - y^(i)
    dw += x^(i)*dz^(i)  # 在代码中采取np.dot(x^(i),dz^(i))
    db += dz^(i)
J /= m  dw /=m  db /= m
dw := dw - αdw  # α为学习率
b := b - αdb
```

### 向量化逻辑回归

原：

改进：

$$
X = \{ x^{(1)},  x^{(2)} \dots  x^{(m)}\} \quad X.shape(): n_x \times m \\\\
\begin{align}
Z = [z^{(1)},  z^{(2)} \dots  z^{(m)}] &= w^TX + [b, b \dots b] \\
&= [w^T x^{(1)} + b, w^T x^{(2)} + b \dots w^T x^{(m)} + b] \\
&= [z^{(1)}, z^{(2)} \dots z^{(m)}]
\end{align}
$$

```python
Z = np.dot(w.T, X) + b
```

$$
A = [a^{(1)}, a^{(2)} \dots a^{(m)}]
$$

### 向量化逻辑回归的梯度输出

$$
dz^{(1)} = a^{(1)} - y^{(1)} \quad dz^{(2)} = a^{(2)} - y^{(2)} \dots \\
dZ = [dz^{(1)}, dz^{(2)} \dots dz^{(m)}] \\
A = [a^{(1)}, a^{(2)} \dots a^{(m)}] \quad Y = [y^{(1)}, y^{(2)} \dots y^{(m)}] \\
dZ  = A - Y = [a^{(1)} - y^{(1)}, a^{(2)} - y^{(2)} \dots a^{(m)} - y^{(m)}]
$$

$$
\begin{align}
db &= \frac{1}{m} \sum_{i=1}^{m} dz^{(i)} \\
&= \frac{1}{m} \text{np.sum}(dz)
\end{align}
$$

$$
dw = \frac{1}{m} X dz^T = \frac{1}{m} [x^{(1)} \dots x^{(m)}]
\begin{bmatrix}
    dz^{(1)} \\
    dz^{(2)} \\
    :       \\
    dz^{(m)}
\end{bmatrix}
$$
